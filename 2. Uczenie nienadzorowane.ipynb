{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa71ec99",
   "metadata": {},
   "source": [
    "Przed oddaniem zadania upewnij się, że wszystko działa poprawnie.\n",
    "**Uruchom ponownie kernel** (z paska menu: Kernel$\\rightarrow$Restart) a następnie\n",
    "**wykonaj wszystkie komórki** (z paska menu: Cell$\\rightarrow$Run All).\n",
    "\n",
    "Upewnij się, że wypełniłeś wszystkie pola `TU WPISZ KOD` lub `TU WPISZ ODPOWIEDŹ`, oraz\n",
    "że podałeś swoje imię i nazwisko poniżej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbacc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490ed2b4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b09b0c",
   "metadata": {},
   "source": [
    "# 2. Uczenie nienadzorowane\n",
    "\n",
    "W poprzednim zeszycie zbadaliśmy różne modele grafowych sieci neuronowych w scenariuszu nadzorowanej klasyfikacji wierzchołków. Teraz zajmiemy się tematem uczenia nienadzorowanego dla GNNów, który jak już wcześniej wspomnieliśmy jest trochę bardziej złożony.\n",
    "\n",
    "W jaki sposób powinna być skonstruowana funkcja kosztu? Czy możemy zastosować model autokodera? Jak w takim razie powinien działać dekoder? Jak uwzględnić relacje między wierzchołkami?\n",
    "\n",
    "To tylko kilka pytań, na które należy odpowiedzieć podczas opracowania nienadzorowanego modelu grafowych sieci neuronowych. W ostatnich latach powstało wiele rozwiązań, obejmujących między innymi:\n",
    "- grafowe autokodery (w tym wariacyjne)\n",
    "- uczenie kontrastowe\n",
    "- uczenie samo-nadzorowane\n",
    "\n",
    "W ninejszym zeszycie zbadamy **model grafowego autokodera** jako najprostszego modelu stosowanego w nienadzorowanym uczeniu reprezentacji grafów, a pozostałe obszary omówimy na najbliższym wykładzie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426ff553",
   "metadata": {},
   "source": [
    "## 2.1. Grafowy autokoder\n",
    "W 2016 roku Kipf, autor pracy wprowadzającej architekturę GCN, opublikował również [artykuł](https://arxiv.org/pdf/1611.07308.pdf) w którym pokazał jak wykorzystać GCNa (lub dowolny inny GNNowy model) w znanej nam architekturze autokodera. Jak wiemy taki model składa się z dwóch komponentów:\n",
    "- **kodera** - w tym wypadku koderem jest wybrana przez nas grafowa sieć neuronowa\n",
    "\n",
    "$$\\mathbf{Z} = \\text{GNN}(\\mathbf{X}, \\mathbf{A})$$\n",
    "\n",
    "- **dekodera** - model dekodera na wejściu przyjmuje wyznaczone reprezentacje $\\mathbf{Z}$ a na wyjściu oblicza rekonstrukcję danego obiektu, w naszym przypadku grafu. Jednak co dokładnie powinien odtworzyć taki grafowy dekoder? Strukturę, atrybuty, czy jedno i drugie? W swojej pracy Kipf zaproponował, aby skupić się wyłącznie na strukturze grafu, tzn. dokonać rekonstrukcji krawędzi. Taki wariant również będziemy rozważać na cele tego laboratorium, przy czym inne scenariusze są równie poprawne i w zależności od konkretnego zadania mogą dostarczać lepszych wyników. \n",
    "\n",
    "W celu zbudowania odpowiedniego dekodera strukturalnego musimy określić w jaki sposób będziemy decydować czy istnieje krawędź między parą dowolnych wierzchołków. Najpopularniejszym rozwiązaniem jest wykorzystanie iloczynu skalarnego, podobnie jak w przypadku modelu Node2vec definiowaliśmy podobieństwo wierzchołków w przestrzeni reprezentacji. Tutaj wybór ten jest umotywowany intuicją, że podobne wierzchołki powinny być połączone krawędzią. Dekoder ma zatem postać:\n",
    "\n",
    "$$\\hat{\\mathbf{A}} = \\sigma(\\mathbf{Z}\\mathbf{Z}^T),$$\n",
    "\n",
    "gdzie:\n",
    "- $\\hat{\\mathbf{A}}$ to rekonstrukcja macierzy sąsiedztwa\n",
    "- $\\sigma(\\cdot)$ to sigmoidalna funkcja aktywacji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c66095d",
   "metadata": {},
   "source": [
    "## Zadanie 2.1. (8 pkt)\n",
    "Wykorzystując zaimplementowane w PyTorch-Geometricu modele:\n",
    "- grafowego autokodera `GAE` - [link](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.GAE.html)\n",
    "- dekodera iloczynu skalarnego `InnerProductDecoder` - [link](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.InnerProductDecoder.html)\n",
    "\n",
    "dokończ implementację klasy `UnsupervisedNodeClassificationGNN`. Zastosuj się do komentarzy umieszczonych przy odpowiednich funkcjach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ba78ae",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5ef5644875c31b43eedce8c5e883f28c",
     "grade": true,
     "grade_id": "implement-unsupervised",
     "locked": false,
     "points": 8,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch import nn\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "class UnsupervisedNodeClassificationGNN(pl.LightningModule):\n",
    "    \"\"\"Unsupervised node classification for a given GNN model.\"\"\"\n",
    "\n",
    "    def __init__(self, gnn: nn.Module):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Utwórz model GAE\n",
    "        \n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        self._downstream_model = None\n",
    "        self.training_step_outputs = []\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        edge_index: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        # TODO: Funkcja forward powinna zwracać wektory reprezentacji `z`\n",
    "        \n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def training_step(self, batch: List[Data], batch_idx: int) -> dict:\n",
    "        data = batch[0]\n",
    "\n",
    "        z = ...\n",
    "        loss = ...\n",
    "\n",
    "        # TODO: Wyznacz wektory reprezentacji `z` oraz oblicz funkcję kosztu `loss`\n",
    "        \n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        self.log(\"step\", self.trainer.current_epoch)\n",
    "        self.log(\"train/loss\", loss.item(), on_epoch=True, on_step=False)\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"z_train\": z[data.train_mask],\n",
    "            \"y_train\": data.y[data.train_mask],\n",
    "        }\n",
    "    \n",
    "    def on_train_batch_end(self, outputs, batch, batch_idx) -> None:\n",
    "        self.training_step_outputs.append(outputs)\n",
    "    \n",
    "    def on_validation_epoch_start(self) -> None:\n",
    "        z_train = torch.cat([out[\"z_train\"].detach() for out in self.training_step_outputs], dim=0)\n",
    "        y_train = torch.cat([out[\"y_train\"] for out in self.training_step_outputs], dim=0)\n",
    "        self.training_step_output = []\n",
    "        \n",
    "        auc = ...\n",
    "\n",
    "        # TODO: Naucz model regresji logistycznej na parach (`z_train`, `y_train`), \n",
    "        # a następnie oblicz wartość miary AUC na zbiorze treningowym. Wykorzystaj\n",
    "        # model regresji logistycznej z biblioteki Scikit-Learn i przypisz go do\n",
    "        # pola: `self._downstream_model`.\n",
    "        \n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        self.log(\"train/auc\", auc, on_epoch=True, on_step=False)\n",
    "\n",
    "    def validation_step(self, batch: List[Data], batch_idx: int):\n",
    "        data = batch[0]\n",
    "\n",
    "        auc = self._compute_auc(data=data, mask=data.val_mask)\n",
    "\n",
    "        self.log(\"step\", self.trainer.current_epoch)\n",
    "        self.log(\"val/auc\", auc, on_epoch=True, on_step=False)\n",
    "\n",
    "        return {\"auc\": auc}\n",
    "\n",
    "    def test_step(self, batch: List[Data], batch_idx: int):\n",
    "        data = batch[0]\n",
    "\n",
    "        auc = self._compute_auc(data=data, mask=data.test_mask)\n",
    "\n",
    "        self.log(\"step\", self.trainer.current_epoch)\n",
    "        self.log(\"test/auc\", auc, on_epoch=True, on_step=False)\n",
    "\n",
    "        return {\"auc\": auc}\n",
    "\n",
    "    def predict_step(\n",
    "        self,\n",
    "        batch: List[Data],\n",
    "        batch_idx: int,\n",
    "        dataloader_idx: Optional[int] = None,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        data = batch[0]\n",
    "\n",
    "        z = self(data.x, data.edge_index)\n",
    "        y = data.y\n",
    "\n",
    "        return z, y\n",
    "\n",
    "    def _compute_auc(self, data: Data, mask: torch.Tensor) -> float:\n",
    "        # TODO: Oblicz wartość miary AUC dla zadanego przez maskę\n",
    "        # podzbioru wierzchołków.\n",
    "        \n",
    "        auc = ...\n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        return auc\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(\n",
    "            params=self.parameters(),\n",
    "            lr=1e-3,\n",
    "            weight_decay=5e-4,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368f4af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import GraphData\n",
    "\n",
    "\n",
    "datamodule = GraphData(dataset_name=\"Cora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e4ace8",
   "metadata": {},
   "source": [
    "Dla wszystkich modeli zdefiniujmy sobie zbiór wspólnych hiperparametrów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8b82e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"num_epochs\": 10,\n",
    "    \"hidden_dim\":  256,\n",
    "    \"emb_dim\": 128,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab47e13",
   "metadata": {},
   "source": [
    "Porównamy teraz jakość działania modeli GNNowych, wprowadzonych w poprzednim zeszycie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a2d3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "from src.trainer import get_default_trainer\n",
    "from src.utils import visualize_embeddings\n",
    "\n",
    "\n",
    "class GCNModel(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.conv2 = GCNConv(hidden_dim, out_dim)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        z = self.act1(self.conv1(x, edge_index))\n",
    "        z = self.act2(self.conv2(z, edge_index))\n",
    "        return z\n",
    "\n",
    "\n",
    "class GraphSAGEModel(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_dim, hidden_dim)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.conv2 = SAGEConv(hidden_dim, out_dim)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        z = self.act1(self.conv1(x, edge_index))\n",
    "        z = self.act2(self.conv2(z, edge_index))\n",
    "        return z\n",
    "\n",
    "\n",
    "class GATModel(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(in_dim, hidden_dim, heads=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.conv2 = GATConv(hidden_dim, out_dim, heads=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        z = self.act1(self.conv1(x, edge_index))\n",
    "        z = self.act2(self.conv2(z, edge_index))\n",
    "        return z\n",
    "\n",
    "    \n",
    "def evaluate_unsupervised_models():\n",
    "    scenarios = [\n",
    "        (\"GCN\", GCNModel),\n",
    "        (\"GraphSAGE\", GraphSAGEModel),\n",
    "        (\"GAT\", GATModel),\n",
    "    ]\n",
    "    \n",
    "    for model_name, gnn_cls in scenarios:\n",
    "        gnn = gnn_cls(\n",
    "            in_dim=datamodule.num_node_features,\n",
    "            hidden_dim=hparams[\"hidden_dim\"],\n",
    "            out_dim=hparams[\"emb_dim\"],\n",
    "        )\n",
    "    \n",
    "        model=UnsupervisedNodeClassificationGNN(gnn=gnn)\n",
    "\n",
    "        trainer = get_default_trainer(\n",
    "            num_epochs=hparams[\"num_epochs\"],\n",
    "            model_name=f\"unsupervised_{model_name}\",\n",
    "        )\n",
    "    \n",
    "        trainer.fit(model=model, datamodule=datamodule)\n",
    "    \n",
    "        test_auc = trainer.test(model=model, datamodule=datamodule, verbose=False)[0][\"test/auc\"]\n",
    "        z, y = trainer.predict(model=model, datamodule=datamodule)[0]\n",
    "    \n",
    "        fig = visualize_embeddings(z=z, y=y)\n",
    "        fig.suptitle(f\"Unsupervised {model_name} - test AUC: {test_auc * 100.:.2f} [%]\")\n",
    "    \n",
    "        plt.show()\n",
    "    \n",
    "\n",
    "evaluate_unsupervised_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0425c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./data/logs/ --port 6006"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
